import torch
import torch.nn as nn
import torch.nn.functional as F
import time
import numpy as np
from typing import Tuple, Optional, Union
import matplotlib.pyplot as plt
import warnings
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# --- Amorphous Operations (Complete Suite) ---
class AmorphousTorchOps:
    @staticmethod
    def amorphous_base_torch(value: torch.Tensor, base: torch.Tensor, fluidity: float = 0.1) -> torch.Tensor:
        base = torch.clamp(base.to(value.device), min=1.1)
        base_variation = torch.randn_like(value) * fluidity * base
        effective_base = torch.clamp(base + base_variation, min=1.1)
        value_safe = torch.sign(value) * torch.clamp(torch.abs(value), min=1e-9)
        log_value = torch.log(torch.abs(value_safe))
        log_base = torch.log(effective_base)
        return torch.sign(value) * (log_value / log_base)

    @staticmethod
    def amorphous_modulus_torch(dividend: torch.Tensor, divisor: torch.Tensor, fluidity: float = 0.1) -> torch.Tensor:
        divisor = torch.clamp(divisor.to(dividend.device), min=0.1)
        divisor_variation = torch.randn_like(dividend) * fluidity * divisor
        effective_divisor = torch.clamp(divisor + divisor_variation, min=1e-9)
        return dividend % effective_divisor

    @staticmethod
    def anamorphous_reconstruction(original_input: torch.Tensor, amorphous_component: torch.Tensor, fluidity: float, distortion_factor: float) -> torch.Tensor:
        distortion = 1 + torch.randn_like(amorphous_component) * fluidity * distortion_factor
        # A stable combination of original signal and distorted amorphous signal
        return original_input + amorphous_component * distortion

# --- Layer Implementations (Final Versions) ---
class FullyAmorphousQFRL(nn.Module):
    """The ultimate QFRL using the full suite of amorphous operations."""
    def __init__(self, in_features, out_features, num_quantum_states=4, amorphous_fluidity=0.1):
        super().__init__()
        self.amorphous_fluidity = amorphous_fluidity
        self.input_projection = nn.Linear(in_features, out_features)
        self.quantum_weights = nn.Parameter(torch.randn(num_quantum_states, out_features, out_features))
        self.fractal_scales = nn.Parameter(torch.randn(out_features, out_features))
        self.adaptive_base = nn.Parameter(torch.tensor(2.5))
        self.adaptive_divisor = nn.Parameter(torch.tensor(np.pi))
        self.output_projection = nn.Linear(out_features, out_features)
        self.quantum_gate = nn.Parameter(torch.tensor(0.1))
        self.fractal_gate = nn.Parameter(torch.tensor(0.1))

    def forward(self, x: torch.Tensor, epoch_ratio: float = 1.0) -> torch.Tensor:
        current_fluidity = self.amorphous_fluidity * epoch_ratio
        
        x_proj = self.input_projection(x)
        x_proj = F.gelu(x_proj)
        x_norm = F.layer_norm(x_proj, x_proj.shape[-1:])
        
        # Amorphous Base Transformation
        x_transformed = AmorphousTorchOps.amorphous_base_torch(x_norm, self.adaptive_base, current_fluidity)
        
        # Amorphous Modulus for Quantum Weights
        quantum_states = torch.randint(0, self.quantum_weights.size(0), (x.size(0),), device=x.device)
        base_weights = torch.index_select(self.quantum_weights, 0, quantum_states)
        weights = AmorphousTorchOps.amorphous_modulus_torch(base_weights, self.adaptive_divisor, current_fluidity)
        x_quantum = torch.bmm(x_transformed.unsqueeze(1), weights).squeeze(1)

        # Anamorphous Reconstruction for Fractal Path
        fractal_base = torch.sin(torch.matmul(x_norm, self.fractal_scales))
        x_fractal = AmorphousTorchOps.anamorphous_reconstruction(
            original_input=fractal_base,
            amorphous_component=x_transformed, # Use amorphous signal as distortion source
            fluidity=current_fluidity,
            distortion_factor=0.5
        )

        x_combined = (self.quantum_gate * x_quantum) + (self.fractal_gate * x_fractal) + x_proj
        return self.output_projection(x_combined)

class StandardLayer(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(in_features, out_features), nn.GELU(), nn.Linear(out_features, out_features))
    def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor: return self.net(x)

# --- Model and Data Setup ---
class Classifier(nn.Module):
    def __init__(self, layer_type, input_size=2, hidden_size=128, num_classes=2):
        super().__init__()
        self.layer_type = layer_type
        if layer_type == "fully_amorphous": self.layer = FullyAmorphousQFRL(input_size, hidden_size)
        else: self.layer = StandardLayer(input_size, hidden_size)
        self.classifier = nn.Linear(hidden_size, num_classes)
        
    def forward(self, x, epoch_ratio: float = 1.0):
        if self.layer_type != "standard":
            x = self.layer(x, epoch_ratio=epoch_ratio)
        else:
            x = self.layer(x)
        return self.classifier(x)

def create_spirals_dataset(n_samples, noise=0.3):
    n = n_samples // 2
    t = np.sqrt(np.random.rand(n, 1)) * 900 * (2*np.pi)/360
    dx = -np.cos(t)*t + np.random.rand(n, 1) * noise
    dy = np.sin(t)*t + np.random.rand(n, 1) * noise
    return (torch.FloatTensor(np.vstack((np.hstack((dx, dy)), np.hstack((-dx, -dy))))),
            torch.LongTensor(np.hstack((np.zeros(n), np.ones(n)))))

# --- Analysis Functions ---
def run_training_and_evaluation(model, train_loader, val_loader, epochs, device):
    model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss()
    history = {'val_acc': [], 'adaptive_base': [], 'adaptive_divisor': [], 'quantum_gate': [], 'fractal_gate': []}

    print(f"\n--- Training {model.layer_type} model for {epochs} epochs ---")
    for epoch in range(epochs):
        model.train()
        for data, targets in train_loader:
            optimizer.zero_grad()
            output = model(data.to(device), epoch_ratio=(epoch + 1) / epochs)
            loss = criterion(output, targets.to(device))
            loss.backward()
            optimizer.step()
        
        model.eval()
        correct, total = 0, 0
        with torch.no_grad():
            for data, targets in val_loader:
                output = model(data.to(device), epoch_ratio=1.0)
                correct += (output.argmax(1) == targets.to(device)).sum().item()
                total += targets.size(0)
        
        val_acc = correct / total
        history['val_acc'].append(val_acc)
        
        log_str = f"Epoch {epoch+1:3d}/{epochs} -> Val Acc: {val_acc:.2%}"
        if hasattr(model.layer, 'adaptive_base'):
            base_val = model.layer.adaptive_base.item()
            div_val = model.layer.adaptive_divisor.item()
            qg_val = model.layer.quantum_gate.item()
            fg_val = model.layer.fractal_gate.item()
            history['adaptive_base'].append(base_val)
            history['adaptive_divisor'].append(div_val)
            history['quantum_gate'].append(qg_val)
            history['fractal_gate'].append(fg_val)
            log_str += f" | Base: {base_val:.2f}, Div: {div_val:.2f}, QG: {qg_val:.2f}, FG: {fg_val:.2f}"
        
        if (epoch + 1) % 10 == 0:
            print(log_str)

    all_preds, all_labels = [], []
    with torch.no_grad():
        for data, targets in val_loader:
            output = model(data.to(device))
            all_preds.extend(output.argmax(1).cpu().numpy())
            all_labels.extend(targets.cpu().numpy())
    return history, (all_labels, all_preds)

def plot_dashboard(histories, cms, models, X_val, y_val):
    fig = plt.figure(figsize=(16, 16))
    gs = fig.add_gridspec(3, 2)
    fig.suptitle("Final Amorphous Model Analysis", fontsize=20, weight='bold')

    # Plots
    ax_acc = fig.add_subplot(gs[0, 0]); ax_params = fig.add_subplot(gs[0, 1])
    for l, h in histories.items():
        ax_acc.plot(h['val_acc'], label=f'{l}')
        if h['adaptive_base']: 
            ax_params.plot(h['adaptive_base'], label=f'{l} Base', color='blue')
            ax_params.plot(h['adaptive_divisor'], label=f'{l} Divisor', color='cyan', linestyle='--')
        if h['quantum_gate']:
            ax_params.plot(h['quantum_gate'], label=f'{l} Q-Gate', color='red')
            ax_params.plot(h['fractal_gate'], label=f'{l} F-Gate', color='magenta', linestyle=':')
    
    ax_acc.set_title("Validation Accuracy"); ax_acc.legend(); ax_acc.grid(True, alpha=0.5)
    ax_params.set_title("Learnable Parameter Evolution"); ax_params.legend(); ax_params.grid(True, alpha=0.5)
    
    # Confusion Matrices & Decision Boundaries
    for i, (l_type, model) in enumerate(models.items()):
        ax_cm = fig.add_subplot(gs[1, i]); ax_db = fig.add_subplot(gs[2, i])
        disp = ConfusionMatrixDisplay(confusion_matrix=cms[l_type], display_labels=[0, 1])
        disp.plot(ax=ax_cm, cmap='viridis'); ax_cm.set_title(f"CM: {l_type}")
        plot_decision_boundary(model, X_val, y_val, ax_db, f"Boundary: {l_type}")

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.savefig('final_analysis_dashboard.png')
    print("\nFinal analysis dashboard saved as 'final_analysis_dashboard.png'")

def plot_decision_boundary(model, X, y, ax, title):
    model.to('cpu').eval()
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))
    with torch.no_grad():
        Z = model(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()).argmax(axis=1).reshape(xx.shape)
    ax.contourf(xx, yy, Z, alpha=0.5, cmap=plt.cm.RdYlBu)
    ax.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k', cmap=plt.cm.RdYlBu)
    ax.set_title(title, weight='bold'); ax.set_xticks(()); ax.set_yticks(())

# --- Main Execution ---
def main():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Running final upgraded tests on device: {device}")
    
    X, y = create_spirals_dataset(2000)
    train_ds, val_ds = torch.utils.data.random_split(TensorDataset(X, y), [0.8, 0.2])
    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=128)

    histories, cms, models = {}, {}, {}
    # We will now only compare the best amorphous layer to the standard one.
    for l_type in ["fully_amorphous", "standard"]:
        model = Classifier(l_type)
        history, cm_data = run_training_and_evaluation(model, train_loader, val_loader, epochs=200, device=device)
        histories[l_type], cms[l_type], models[l_type] = history, confusion_matrix(cm_data[0], cm_data[1]), model

    X_val, y_val = val_ds.dataset.tensors[0][val_ds.indices], val_ds.dataset.tensors[1][val_ds.indices]
    plot_dashboard(histories, cms, models, X_val, y_val)

if __name__ == '__main__':
    main()
